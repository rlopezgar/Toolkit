{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    " <a href=\"http://cocl.us/pytorch_link_top\"><img src=\"http://cocl.us/Pytorch_top\" width=\"950,\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200,\" align=\"center\">\n",
    "\n",
    "\n",
    "<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines \n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li><a href=\"#ref0\">Helper functions </a></li>\n",
    "\n",
    "<li><a href=\"#ref1\"> Prepare Data </a></li>\n",
    "<li><a href=\"#ref2\">Convolutional Neral Network </a></li>\n",
    "<li><a href=\"#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n",
    "<li><a href=\"#ref4\">Analyse Results</a></li>\n",
    "\n",
    "<br>\n",
    "<p></p>\n",
    "Estimated Time Needed: <strong>25 min</strong>\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref0\"></a>\n",
    "<h2 align=center>Helper functions </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb972f32490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to plot out the parameters of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    #number of output channels \n",
    "    n_out=W.shape[0]\n",
    "    #number of input channels \n",
    "    n_in=W.shape[1]\n",
    "    w_min=W.min().item()\n",
    "    w_max=W.max().item()\n",
    "    fig, axes = plt.subplots(n_out,n_in)\n",
    "    fig.subplots_adjust(hspace = 0.1)\n",
    "    out_index=0\n",
    "    in_index=0\n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "    \n",
    "        if in_index>n_in-1:\n",
    "            out_index=out_index+1\n",
    "            in_index=0\n",
    "              \n",
    "        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index=in_index+1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>show_data</code>: plot out data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def show_data(dataset,sample):\n",
    "\n",
    "    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n",
    "    plt.title('y='+str(dataset.y[sample].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some toy data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n",
    "        \"\"\"\n",
    "        p:portability that pixel is wight  \n",
    "        N_images:number of images \n",
    "        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n",
    "        \"\"\"\n",
    "        if train==True:\n",
    "            np.random.seed(1)  \n",
    "        \n",
    "        #make images multiple of 3 \n",
    "        N_images=2*(N_images//2)\n",
    "        images=np.zeros((N_images,1,11,11))\n",
    "        start1=3\n",
    "        start2=1\n",
    "        self.y=torch.zeros(N_images).type(torch.long)\n",
    "\n",
    "        for n in range(N_images):\n",
    "            if offset>0:\n",
    "        \n",
    "                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n",
    "                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n",
    "            else:\n",
    "                low=4\n",
    "                high=1\n",
    "        \n",
    "            if n<=N_images//2:\n",
    "                self.y[n]=0\n",
    "                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n",
    "            elif  n>N_images//2:\n",
    "                self.y[n]=1\n",
    "                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n",
    "           \n",
    "        \n",
    "        \n",
    "        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "        del(images)\n",
    "        np.random.seed(0)\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(A,number_rows= 1,name=\"\"):\n",
    "    A=A[0,:,:,:].detach().numpy()\n",
    "    n_activations=A.shape[0]\n",
    "    \n",
    "    \n",
    "    print(n_activations)\n",
    "    A_min=A.min().item()\n",
    "    A_max=A.max().item()\n",
    "\n",
    "    if n_activations==1:\n",
    "\n",
    "        # Plot the image.\n",
    "        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "        fig.subplots_adjust(hspace = 0.4)\n",
    "        for i,ax in enumerate(axes.flat):\n",
    "            if i< n_activations:\n",
    "                # Set the label for the sub-plot.\n",
    "                ax.set_xlabel( \"activation:{0}\".format(i+1))\n",
    "\n",
    "                # Plot the image.\n",
    "                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    #by Duane Nielsen\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "<h2 align=center>Prepare Data </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset with 10000 samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "N_images=10000\n",
    "train_dataset=Data(N_images=N_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Data at 0x7fb96fd66910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset=Data(N_images=1000,train=False)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the data type is long \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the third label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZn0lEQVR4nO3df2zU9R3H8de1pdeOtMeP2paOIpWRVAEZ2sKQZXOhkRFCYIu/FhgMly0hRSidm7ClEINQwIkLwkD5g7FQfs2t6EjYJBUhRCiFUtE4Sx1kVrD8yPSu/Dpc77M/Fo90lB/S7/V9V56P5PtHv9/jvm++KffM93tf7nzOOScAALpYkvUAAIA7EwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEBAnDp58qQef/xx9erVS5mZmZo0aZKOHz9uPRbgGR+fBQfEn/Pnz+uBBx5QMBjUL37xC/Xo0UMvvfSSnHNqaGhQ3759rUcEOi3FegAA1/r973+vpqYmHTx4UMXFxZKk8ePHa+jQoXrxxRe1ZMkS4wmBzuMSHHAbdu/eLZ/Pp+rq6mu2bdq0ST6fT/v377/t53/ttddUXFwcjY8kFRYWauzYsdq2bdttPy8QTwgQcBsefvhh5efnq6qq6pptVVVVGjRokEaPHq1wOKxz587d0vKlSCSio0ePqqio6JrnHjlypP75z3+qtbU1pn8/oCsQIOA2+Hw+TZ06VTt27FAwGIyuP3v2rN58801NnTpVkrR582bdddddt7R86d///rfC4bD69et3zX6/XHfq1KkY/w2B2OM9IOA2TZs2TZWVlXrttdf005/+VJK0detW/ec//4kGaNy4cdq1a9dXet5Lly5Jkvx+/zXb0tLS2j0GSGQECLhNhYWFKi4uVlVVVTRAVVVV+ta3vqVvfOMbkv53xtLRmcyNpKenS5LC4fA12y5fvtzuMUAiI0BAJ0ybNk1z5szRJ598onA4rAMHDmjVqlXR7ZcuXWp3ie5GcnNzJUl9+vSR3+/Xp59+es1jvlyXl5fnwfSALQIEdMKTTz6p8vJybd68WZcuXVKPHj30xBNPRLdv3bpVM2bMuKXn+vK/5CUlJWnYsGE6dOjQNY+pra3VPffco4yMDG/+AoAhAgR0QlZWlsaPH6+NGzfq8uXL+v73v6+srKzo9tt5D0iSHn30Uc2bN0+HDh2K3g3X2Niot956S88884xn8wOW+CQEoJP+/Oc/69FHH5X0vzOexx9/vNPP2draqhEjRqi1tVXPPPOMevTooRUrVqitrU0NDQ3t7poDEhUBAjrpypUrys3NVSQSUUtLS/ROtc765JNPNHfuXL355puKRCJ6+OGH9dJLL0VvcAASHZfggE5KSkpSSkqKJk6c6Fl8JKl///7605/+5NnzAfGG/4gKdNL27dt19uxZTZs2zXoUIKFwCQ64TbW1tTp69KgWLVqkrKws1dfXW48EJBTOgIDbtGbNGs2cOVPZ2dn64x//aD0OkHA4AwIAmOAMCABgggABAEzE3W3YkUhEp06dUkZGhnw+n/U4AICvyDmn1tZW5eXlKSnp+uc5cRegU6dOKT8/33oMAEAnNTc3q3///tfdHneX4PiQRQDoHm72eh53AeKyGwB0Dzd7PY+7AAEA7gwECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCJmAVq9erUGDhyotLQ0jRo1SgcPHozVrgAACSgmAdq6davKy8u1cOFC1dfXa/jw4Ro3bpzOnDkTi90BABJQTL6Se9SoUSouLtaqVask/e87fvLz8/X0009r3rx57R4bDocVDoejP4dCIb6OAQC6gWAwqMzMzOtu9/wM6MqVKzp8+LBKSkqu7iQpSSUlJdq/f/81j6+srFQgEIguxAcA7gyeB+jcuXNqa2tTTk5Ou/U5OTlqaWm55vHz589XMBiMLs3NzV6PBACIQ+bfiOr3++X3+63HAAB0Mc/PgLKyspScnKzTp0+3W3/69Gnl5uZ6vTsAQILyPECpqal68MEHVVNTE10XiURUU1Oj0aNHe707AECCiskluPLyck2fPl1FRUUaOXKkfve73+nChQuaMWNGLHYHAEhAMQnQE088obNnz2rBggVqaWnRN7/5Tf3tb3+75sYEAMCdKyb/D6gzQqGQAoGA9RgAgE7q8v8HBADArSBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJgw/zoG4FbF2Yd2QJLP57MeAQmMMyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATKRYDwAkGp/PZz2CJMk5Zz0C0CmcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCE5wGqrKxUcXGxMjIylJ2drcmTJ6uxsdHr3QAAEpznAdqzZ49KS0t14MAB7dq1S1988YUeeeQRXbhwwetdAQASmM/F+EtFzp49q+zsbO3Zs0ff+c53bvr4UCikQCAQy5GQoOLl+2/4PqCr4uVYID4Fg0FlZmZed3vMv5AuGAxKkvr06dPh9nA4rHA4HP05FArFeiQAQByI6U0IkUhEZWVlGjNmjIYOHdrhYyorKxUIBKJLfn5+LEcCAMSJmF6Cmzlzpnbu3Kl9+/apf//+HT6mozMgIoSOxMMlJyl+LjvFw/GIl2OB+GR2CW7WrFnasWOH9u7de934SJLf75ff74/VGACAOOV5gJxzevrpp1VdXa23335bBQUFXu8CANANeB6g0tJSbdq0Sa+//royMjLU0tIiSQoEAkpPT/d6dwCABOX5e0DXuya8fv16/eQnP7npn+c2bFxPPLznIcXP+x7xcDzi5VggPnX5e0Dx8I8CABD/+Cw4AIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiZh/IR3Q3fBpH4A3OAMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwEfMALV26VD6fT2VlZbHeFQAggcQ0QHV1dXrllVd0//33x3I3AIAEFLMAnT9/XlOmTNG6devUu3fvWO0GAJCgYhag0tJSTZgwQSUlJTd8XDgcVigUarcAALq/lFg86ZYtW1RfX6+6urqbPrayslLPPfdcLMYAAMQxz8+AmpubNWfOHFVVVSktLe2mj58/f76CwWB0aW5u9nokAEAc8jnnnJdPuH37dv3gBz9QcnJydF1bW5t8Pp+SkpIUDofbbft/oVBIgUDAy5HQTXj8qwoP+Hw+6xEQx4LBoDIzM6+73fNLcGPHjtV7773Xbt2MGTNUWFioZ5999obxAQDcOTwPUEZGhoYOHdpuXc+ePdW3b99r1gMA7lx8EgIAwITn7wF1Fu8B4Xri7FcV4j0g3NjN3gPiDAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAiJt8HBCD2+BQCJDrOgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZSrAcAcHucc9YjyOfzWY+ABMYZEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgIiYBOnnypKZOnaq+ffsqPT1dw4YN06FDh2KxKwBAgvL807A/++wzjRkzRt/73ve0c+dO3XXXXWpqalLv3r293hUAIIF5HqBly5YpPz9f69evj64rKCjwejcAgATn+SW4N954Q0VFRXrssceUnZ2tESNGaN26ddd9fDgcVigUarcAALo/zwN0/PhxrVmzRoMHD9bf//53zZw5U7Nnz9aGDRs6fHxlZaUCgUB0yc/P93okAEAc8jmPv1YxNTVVRUVFeuedd6LrZs+erbq6Ou3fv/+ax4fDYYXD4ejPoVCICKFD8fANoGiPb0TFjQSDQWVmZl53u+dnQP369dN9993Xbt29996rjz/+uMPH+/1+ZWZmtlsAAN2f5wEaM2aMGhsb2607duyY7r77bq93BQBIYJ4HaO7cuTpw4ICWLFmijz76SJs2bdKrr76q0tJSr3cFAEhgnr8HJEk7duzQ/Pnz1dTUpIKCApWXl+tnP/vZLf3ZUCikQCDg9UjoBngPKP7wHhBu5GbvAcUkQJ1BgHA9cfarChEg3FiX34QAAMCtIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCE5wFqa2tTRUWFCgoKlJ6erkGDBmnRokVyznm9KwBAAkvx+gmXLVumNWvWaMOGDRoyZIgOHTqkGTNmKBAIaPbs2V7vDgCQoDwP0DvvvKNJkyZpwoQJkqSBAwdq8+bNOnjwoNe7AgAkMM8vwT300EOqqanRsWPHJEnvvvuu9u3bp/Hjx3f4+HA4rFAo1G4BAHR/np8BzZs3T6FQSIWFhUpOTlZbW5sWL16sKVOmdPj4yspKPffcc16PAQCIc56fAW3btk1VVVXatGmT6uvrtWHDBv32t7/Vhg0bOnz8/PnzFQwGo0tzc7PXIwEA4pDPeXx7Wn5+vubNm6fS0tLouueff14bN27Uhx9+eNM/HwqFFAgEvBwJ3QR3UsYfn89nPQLiWDAYVGZm5nW3e34GdPHiRSUltX/a5ORkRSIRr3cFAEhgnr8HNHHiRC1evFgDBgzQkCFDdOTIEa1YsUJPPfWU17sCACQwzy/Btba2qqKiQtXV1Tpz5ozy8vL0ox/9SAsWLFBqaupN/zyX4HA9XIKLP1yCw43c7BKc5wHqLAKE64mzX1WIAOHGuvw9IAAAbgUBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEynWAwC3yufzWY8AwEOcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDEVw7Q3r17NXHiROXl5cnn82n79u3ttjvntGDBAvXr10/p6ekqKSlRU1OTV/MCALqJrxygCxcuaPjw4Vq9enWH25cvX66VK1dq7dq1qq2tVc+ePTVu3Dhdvny508MCALoR1wmSXHV1dfTnSCTicnNz3QsvvBBd9/nnnzu/3+82b97c4XNcvnzZBYPB6NLc3OwksbCwsLAk+BIMBm/YEE/fAzpx4oRaWlpUUlISXRcIBDRq1Cjt37+/wz9TWVmpQCAQXfLz870cCQAQpzwNUEtLiyQpJyen3fqcnJzotv83f/58BYPB6NLc3OzlSACAOGX+ldx+v19+v996DABAF/P0DCg3N1eSdPr06XbrT58+Hd0GAIDkcYAKCgqUm5urmpqa6LpQKKTa2lqNHj3ay10BABLcV74Ed/78eX300UfRn0+cOKGGhgb16dNHAwYMUFlZmZ5//nkNHjxYBQUFqqioUF5eniZPnuzl3ACARPdVb73evXt3h7fbTZ8+PXordkVFhcvJyXF+v9+NHTvWNTY23vLzB4NB81sHWVhYWFg6v9zsNmyfc84pjoRCIQUCAesxAACdFAwGlZmZed3tfBYcAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACbiLkBx9v9iAQC36Wav53EXoNbWVusRAAAeuNnredx9FE8kEtGpU6eUkZEhn893W88RCoWUn5+v5ubmG34MxJ2AY9Eex+MqjsVVHIurvDgWzjm1trYqLy9PSUnXP88x/0K6/5eUlKT+/ft78lyZmZl3/C/TlzgW7XE8ruJYXMWxuKqzx+JWPtMz7i7BAQDuDAQIAGCiWwbI7/dr4cKF8vv91qOY41i0x/G4imNxFcfiqq48FnF3EwIA4M7QLc+AAADxjwABAEwQIACACQIEADBBgAAAJrplgFavXq2BAwcqLS1No0aN0sGDB61H6nKVlZUqLi5WRkaGsrOzNXnyZDU2NlqPFReWLl0qn8+nsrIy61FMnDx5UlOnTlXfvn2Vnp6uYcOG6dChQ9ZjmWhra1NFRYUKCgqUnp6uQYMGadGiRXfEhyLv3btXEydOVF5ennw+n7Zv395uu3NOCxYsUL9+/ZSenq6SkhI1NTV5OkO3C9DWrVtVXl6uhQsXqr6+XsOHD9e4ceN05swZ69G61J49e1RaWqoDBw5o165d+uKLL/TII4/owoUL1qOZqqur0yuvvKL777/fehQTn332mcaMGaMePXpo586d+uCDD/Tiiy+qd+/e1qOZWLZsmdasWaNVq1bpH//4h5YtW6bly5fr5Zdfth4t5i5cuKDhw4dr9erVHW5fvny5Vq5cqbVr16q2tlY9e/bUuHHjdPnyZe+GcN3MyJEjXWlpafTntrY2l5eX5yorKw2nsnfmzBknye3Zs8d6FDOtra1u8ODBbteuXe673/2umzNnjvVIXe7ZZ5913/72t63HiBsTJkxwTz31VLt1P/zhD92UKVOMJrIhyVVXV0d/jkQiLjc3173wwgvRdZ9//rnz+/1u8+bNnu23W50BXblyRYcPH1ZJSUl0XVJSkkpKSrR//37DyewFg0FJUp8+fYwnsVNaWqoJEya0+/2407zxxhsqKirSY489puzsbI0YMULr1q2zHsvMQw89pJqaGh07dkyS9O6772rfvn0aP3688WS2Tpw4oZaWlnb/VgKBgEaNGuXpa2ncfRp2Z5w7d05tbW3Kyclptz4nJ0cffvih0VT2IpGIysrKNGbMGA0dOtR6HBNbtmxRfX296urqrEcxdfz4ca1Zs0bl5eX69a9/rbq6Os2ePVupqamaPn269Xhdbt68eQqFQiosLFRycrLa2tq0ePFiTZkyxXo0Uy0tLZLU4Wvpl9u80K0ChI6Vlpbq/fff1759+6xHMdHc3Kw5c+Zo165dSktLsx7HVCQSUVFRkZYsWSJJGjFihN5//32tXbv2jgzQtm3bVFVVpU2bNmnIkCFqaGhQWVmZ8vLy7sjj0dW61SW4rKwsJScn6/Tp0+3Wnz59Wrm5uUZT2Zo1a5Z27Nih3bt3e/Y9S4nm8OHDOnPmjB544AGlpKQoJSVFe/bs0cqVK5WSkqK2tjbrEbtMv379dN9997Vbd++99+rjjz82msjWL3/5S82bN09PPvmkhg0bph//+MeaO3euKisrrUcz9eXrZaxfS7tVgFJTU/Xggw+qpqYmui4SiaimpkajR482nKzrOec0a9YsVVdX66233lJBQYH1SGbGjh2r9957Tw0NDdGlqKhIU6ZMUUNDg5KTk61H7DJjxoy55nb8Y8eO6e677zaayNbFixev+cbO5ORkRSIRo4niQ0FBgXJzc9u9loZCIdXW1nr7WurZ7QxxYsuWLc7v97s//OEP7oMPPnA///nPXa9evVxLS4v1aF1q5syZLhAIuLffftt9+umn0eXixYvWo8WFO/UuuIMHD7qUlBS3ePFi19TU5KqqqtzXvvY1t3HjRuvRTEyfPt19/etfdzt27HAnTpxwf/nLX1xWVpb71a9+ZT1azLW2trojR464I0eOOEluxYoV7siRI+5f//qXc865pUuXul69ernXX3/dHT161E2aNMkVFBS4S5cueTZDtwuQc869/PLLbsCAAS41NdWNHDnSHThwwHqkLiepw2X9+vXWo8WFOzVAzjn317/+1Q0dOtT5/X5XWFjoXn31VeuRzIRCITdnzhw3YMAAl5aW5u655x73m9/8xoXDYevRYm737t0dvkZMnz7dOfe/W7ErKipcTk6O8/v9buzYsa6xsdHTGfg+IACAiW71HhAAIHEQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAw8V8Lr9rVXHrOKQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the 3rd  sample \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "### Build a Convolutional Neral Network Class \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input image is 11 x11, the following will change the size of the activations:\n",
    "<ul>\n",
    "<il>convolutional layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>convolutional layer </il>\n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer </il>\n",
    "</ul>\n",
    "\n",
    "with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n",
    "We use the following  lines of code to change the image before we get tot he fully connected layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out)\n",
    "out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out1)\n",
    "out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out2)\n",
    "\n",
    "out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_1=2,out_2=1):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        #first Convolutional layers \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "\n",
    "        #second Convolutional layers\n",
    "        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #max pooling \n",
    "\n",
    "        #fully connected layer \n",
    "        self.fc1=nn.Linear(out_2*7*7,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn1(x)\n",
    "        #activation function \n",
    "        x=torch.relu(x)\n",
    "        #max pooling \n",
    "        x=self.maxpool1(x)\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn2(x)\n",
    "        #activation function\n",
    "        x=torch.relu(x)\n",
    "        #max pooling\n",
    "        x=self.maxpool2(x)\n",
    "        #flatten output \n",
    "        x=x.view(x.size(0),-1)\n",
    "        #fully connected layer\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def activations(self,x):\n",
    "        #outputs activation this is not necessary just for fun \n",
    "        z1=self.cnn1(x)\n",
    "        a1=torch.relu(z1)\n",
    "        out=self.maxpool1(a1)\n",
    "        \n",
    "        z2=self.cnn2(out)\n",
    "        a2=torch.relu(z2)\n",
    "        out=self.maxpool2(a2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return z1,a1,z2,a2,out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model=CNN(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the model parameters with the object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "cost=0\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    cost=0    \n",
    "    for x, y in train_loader:\n",
    "      \n",
    "\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss \n",
    "        loss=criterion(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        cost+=loss.item()\n",
    "    cost_list.append(cost)\n",
    "        \n",
    "        \n",
    "    correct=0\n",
    "    #perform a prediction on the validation  data  \n",
    "    for x_test, y_test in validation_loader:\n",
    "\n",
    "        z=model(x_test)\n",
    "        _,yhat=torch.max(z.data,1)\n",
    "\n",
    "        correct+=(yhat==y_test).sum().item()\n",
    "        \n",
    "\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"ref3\"></a>\n",
    "<h2 align=center>Analyse Results</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy on the validation data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of the parameters for the Convolutional layers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the activations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\n",
    "out=model.activations(train_dataset[0][0].view(1,1,11,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[0],number_rows=1,name=\" feature map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[3],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the output of the activation after flattening  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out[4][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do the same for a sample  where y=0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot( out1, 'b')\n",
    "plt.title('Flatted Activation Values  ')\n",
    "plt.ylabel('Activation')\n",
    "plt.xlabel('index')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out0, 'r')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Authors:  \n",
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2023-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n",
    "\n",
    "Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2023-01-01) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <hr>\n",
    "Copyright &copy; 2018 [cognitiveclass.ai](cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
